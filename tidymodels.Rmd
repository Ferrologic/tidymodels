---
title: "Det enklaste sättet att bygga ML-modeller hittills?"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Det finns en uppsjö av sätt att bygga Machine Learning-modeller på. På mina uppdrag, och inte minst under tiden som instruktör på vårt Data Scientist-program, har jag stött på flera av dem. Många, som Sci-kit Learn i Python och Caret är otroligt väl skriven mjukvara. Men gemensamt för båda dessa paket, som sannolikt är de mest populära för Machine Learning i open source-världen, är att de byggts bit efter bit. De har utvecklats parallellt med att det de senaste 10 åren kommit många nya idéer om hur man man bygger robusta ML-modeller. Paketen har byggts över många iterationer och dessutom gjorts om flera gånger. Scikit-learn började som ett projekt på Google Summercamp och Caret har utvecklats för att passa hur man arbetade med non-clinical trials på Pfizer. 

Mot den bakgrunden kändes det ganska rimligt när Max Kuhn, som ligger bakom Caret, annonserade för några år sedan att de har börjat med en ny samling paket för att göra Machine Learning i R. Samlingen heter `tidymodels` och bygger på det populära syntaxet i paketsamlingen `tidyverse`. `tidymodels` skiljer sig från `caret` och `scikit-learn` framför allt genom att det bygger på en uppsättning paket istället för ett paket som löser allt åt en. Det här kan tyckas motsägelsefullt, borde det inte vara enklare att ha ett paket som gör allt åt oss? Men Machine Learning är sällan så enkelt som det ibland framställs. Varje ML-projekt har särskilda behov och ju mer paketerad en lösning är desto svårare blir den att skräddarsy. Med tanke på hur många olika steg en ML-process går igenom känns det rimligt att dela upp den och på så sätt göra den mer modulär.

Paketen som ingår i `tidymodels` är:

```{r}
library(tidymodels)
```

- `rsample` för att dela upp data i tränings- och test-data
- `recipes` för *feature engineering*
- `parsnip` för modellspecificering
- `broom` för att göra modellobjekt till data.frames och tabeller
- `infer` för statistisk inferens
- `yardstick` för att modellprestanda
- `dials` för att arbeta med 
- `tune` och `dials` för tuning av hyperparametrar
- `workflows` för att enkelt kunna bygga ML-workflows

Förutom dessa ingår flera av `tidyverse`-paketen också, som `dplyr`, `ggplot2`, `tibble` och `purrr`.

Det kanske låter som många paket, men det är inte alltid man använder alla och ofta är det bara en eller två funktioner som man anropar.

Nog pratat, låt oss gå igenom ett exempel på hur det funkar.

Vi kan utgå från ett dataset som handlar om pingviner på Antarktis. Målet med vår modell är att kunna prediktera pingvinart utifrån en uppsättning variabler.

```{r penguins, fig.cap="Illustration och paket med pingvindata av Allison Horst: https://github.com/allisonhorst/palmerpenguins"}
knitr::include_graphics("lter_penguins.png")
```

Vi importerar data från paketet `palmerpenguins` som finns på Github. 

```{r import_data, message=FALSE, warning=FALSE}
library(tidyverse)
penguins <- na.omit(palmerpenguins::penguins)

glimpse(penguins)
```


Det första vi gör i en Supervised Machine Learning, är att dela upp data i tränings- och testdata. Idén är, som ni säkert vet, att slumpmässigt ta bort en del av data och spara till sist i modelleringsprocessen för att ha tillgång till "ny" okänd data när vi väl är klara med vår modell. På så sätt får vi en ungefärlig uppfattning om hur modellen kommer att prestera i produktion. 

Förut brukade jag alltid visualisera och utforska all data innan själva modelleringsprocessen började. Men eftersom poängen är att testdata ska representera ny okänd data så har jag fått lära mig att det är klokt att göra all explorativ analys endast på träningsdata.

För att dela upp data använder vi som sagt `rsample()`. Eftersom att data är relativt välbalanserat och observationerna ganska få tar vi en lite större del som test-data. Default-split är `0.75`.

```{r}
set.seed(523)
penguin_split <- initial_split(penguins, prop = 0.6)

penguin_split
```

För att göra vårt `split`-objekt till tränings och test-data använder vi funktionerna `training()` respektive `testing()`.

```{r}
train <- training(penguin_split)
test <- testing(penguin_split)
```

Visualiserar vi näbbredd och näbblängd i träningsdata så ser vi att de är hyfsat distinkta per art.

```{r}
ggplot(train, aes(bill_length_mm, bill_depth_mm, color = species, shape = species)) +
  geom_point() +
  ggthemes::scale_color_colorblind() +
  theme_minimal() +
  labs(
    title = "Näbblängd ~ Näbbredd",
    subtitle = "Palmer Station, Antarktis"
  )
```

Nästa steg i en ML-process är generellt feature engineering. För att göra det med `tidymodels` använder vi `recipes`.

Till att börja med specificerar vi ett "recept" d.v.s. hur vi vill modellera. I det här fallet vill vi använda alla variabler för att predicera `species`.

```{r}
recipe(species ~ ., data = train)
```

I anslutning till receptet vill man generellt göra vissa transformeringar, i `recipes` heter dessa `step_...`, exempelvis: `step_log()`, `step_BoxCox`, `step_naomit` osv. 

När vi är klara med ett recept använder vi funktionen `prep` för att preppa data.

```{r}
recept <- recipe(species ~ ., data = train) %>% 
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  prep()
```

För att få ut data från receptet använder vi funktionen `juice()`.

```{r}
train_recipe <- juice(recept)

train_recipe
```
Vi är nu klara med *feature engineering* och vill träna en modell. Innan vi gör själva träningen av modellen specificerar vi den med `parsnip`.

`parsnip` har stöd för de allra flesta typer av modell och man specificerar vilken typ av modell man vill använda med exempelvis `rand_forest()`, `linear_model()` eller `logistic_reg()`. Sedan specificerar man *engine* med `set_engine()`. På så sätt kan man specificera en modell men välja olika backends för hur den ska tränas. Exempelvis om man vill använda två olika paket för en Random Forest eller om man vill träna en regression första lokalt i R och sedan på ett kluster via Spark.

Om vi dessutom vill tune:a en modell, alltså träna exempelvis en randomforest med olika många träd, mtry osv så kan vi bara skriva `trees = tune()` likt nedan, så kommer vi sedan kunna tune:a.

```{r}
rf <- rand_forest(trees = tune(), mode = "classification") %>% 
  set_engine("ranger")

rf
```

Men vill vi träna modellen i Spark kan vi bara ändra engine.

```{r}
rf_spark <- rand_forest(trees = tune(), mode = "classification") %>% 
  set_engine("spark")

rf_spark
```

I nästa steg använder vi `workflow` för att specificera processen för modelleringen. Vi lägger också till `set_args()` där vi säger att vi vill tune:a `trees`. 

```{r}
wf <- workflow() %>% 
  add_recipe(recept) %>% 
  add_model(rf)
```

Att skapa en 5-fold korsvalidering gör vi med `vfold_cv()` från `rsample`.

```{r}
penguin_folds <- vfold_cv(train, 5)

penguin_folds
```

Nu har det blivit dags att tune:a modellen.

```{r}
hyper_parameters <- tune::tune_grid(wf, resamples = penguin_folds)

collect_metrics(hyper_parameters)
```
Vi kan enkelt extrahera den bästa modellen med `select_best()`.

```{r}
best_hp <- select_best(hyper_parameters, metric = "roc_auc")

best_hp
```

```{r}
best_workflow <- tune::finalize_workflow(wf, best_hp)

best_workflow
```

När vi nu tränar modellen gör vi det på varje fold för att sedan predicera nästa fold. På så sätt kan vi räkna ut träffsäkerhet utan att behöva använda oss av test-data.

I `fit_resamples()` kan vi också specificera våra accuracy-metrics.

```{r}
set.seed(234)
rf_res <- best_workflow %>%
    fit_resamples(
        penguin_folds,
        metrics = metric_set(roc_auc, accuracy, precision, f_meas),
        control = control_resamples(save_pred = TRUE)
    )
```

För att få ut våra prestandamått använder vi bara `collect_metrics()`.

```{r}
collect_metrics(rf_res)
```

Det sista vi gör är att träna modellen på all data och se hur den presterar på test-data. Det gör vi med funktionen `last_fit()`.

```{r}
final_fit <- last_fit(object = best_workflow,
                      split = penguin_split,
                      metrics = metric_set(roc_auc, accuracy, precision, f_meas))

collect_metrics(final_fit)
```

Till sist tränar vi modellen på all data och extraherar den till ett modellobjekt. Detta kan vi sedan använda för att sätta modellen i produktion.

```{r}
best_model <- extract_model(fit(best_workflow, penguins))

best_model
```

















































